{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-22 00:15:27.710879: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-22 00:15:27.823860: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-22 00:15:28.372620: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-22 00:15:28.372680: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-22 00:15:28.372686: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pickle\n",
    "from time import time\n",
    "from typing import List, Dict, Set, Tuple\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import gensim\n",
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from ge.classify import read_node_label, Classifier\n",
    "from ge import LINE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas_profiling import ProfileReport\n",
    "import tensorflow as tf\n",
    "from gensim import utils\n",
    "from numpy import float32 as REAL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphLoader:\n",
    "    def __init__(self):\n",
    "        self.graph = None\n",
    "        self.title = None\n",
    "        \n",
    "    def build_graph(self, \n",
    "                    dataframe: pd.DataFrame, \n",
    "                    columns: List, \n",
    "                    edge_list: List,\n",
    "                    verbose: bool = True,\n",
    "                    title: str = 'Unnamed'):\n",
    "        self.title = title\n",
    "        t0 = time()\n",
    "        self.graph = nx.Graph(name = self.title)\n",
    "\n",
    "        # Add Nodes to the graph.\n",
    "        for column in columns:\n",
    "            self.graph.add_nodes_from(dataframe[column].values, label=column)\n",
    "\n",
    "        # Add remaining columns as Node attributes. Optional\n",
    "        remaining = dataframe.columns.difference(columns)\n",
    "        for node, data in self.graph.nodes(data=True):\n",
    "            if data[\"label\"] == \"K\":\n",
    "                self.graph.nodes[node][\"K\"] = dataframe.loc[dataframe[\"K\"] == int(node), remaining].squeeze().to_dict()\n",
    "\n",
    "        # Add Edges.\n",
    "        for _, row in dataframe.loc[:, columns].iterrows():\n",
    "            for edge in edge_list:\n",
    "                self.graph.add_edge(row[edge[0]], row[edge[1]])\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"FINISHED in {np.round(time() - t0, 3)} seconds.\")\n",
    "            print(nx.info(self.graph))\n",
    "        \n",
    "        return self.graph\n",
    "\n",
    "    def draw_graph(self, graph: nx.Graph, node_colors: dict, node: str = None, radius: int = 1) -> None:\n",
    "        def assign_colors(graph: nx.Graph) -> List:\n",
    "            # Assign Colors to nodes\n",
    "            colors = []\n",
    "            for n, data in graph.nodes(data=True):\n",
    "                node = data[\"label\"]\n",
    "                colors.append(node_colors.get(node, \"white\"))\n",
    "            return colors\n",
    "\n",
    "        f = plt.figure(figsize = (20,12), facecolor=\"darkgray\")\n",
    "        ax = f.add_subplot()\n",
    "\n",
    "        if not node:    \n",
    "            plt.title(self.title)\n",
    "        else:\n",
    "            plt.title(f\"Ego Graph around the node {node}, (radius={radius})\")\n",
    "            graph = nx.ego_graph(graph, node, radius = radius)\n",
    "        \n",
    "        colors = assign_colors(graph)\n",
    "        nx.draw_networkx(graph, node_size = 300, node_color = colors, with_labels = False)\n",
    "        # Add an empty plot to set custom legends\n",
    "        from matplotlib.lines import Line2D\n",
    "        ax.scatter([],[])\n",
    "        legend_elements = [\n",
    "            Line2D([0], [0], marker='o', color='w', label='K', markerfacecolor = node_colors['K'], markersize=15),\n",
    "            Line2D([0], [0], marker='o', color='w', label='S1', markerfacecolor = node_colors['S1'], markersize=10),\n",
    "            Line2D([0], [0], marker='o', color='w', label='S2', markerfacecolor = node_colors['S2'], markersize=10),\n",
    "            Line2D([0], [0], marker='o', color='w', label='S3', markerfacecolor = node_colors['S3'], markersize=10),\n",
    "            Line2D([0], [0], marker='o', color='w', label='S4', markerfacecolor = node_colors['S4'], markersize=10),\n",
    "            Line2D([0], [0], marker='o', color='w', label='S5', markerfacecolor = node_colors['S5'], markersize=10),\n",
    "            Line2D([0], [0], marker='o', color='w', label='S6', markerfacecolor = node_colors['S6'], markersize=10),\n",
    "\n",
    "        ]\n",
    "        ax.legend(handles=legend_elements, loc='best')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrition_data = 'cbd.csv'\n",
    "df_attrition = pd.read_csv(attrition_data)\n",
    "target_column = 'word'\n",
    "selected_columns = ['K', 'S1', 'S2', 'S3', \\\n",
    "                    'S4', 'S5','S6']\n",
    "\n",
    "df_attrition = df_attrition.loc[:, selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrition = 'test.csv'\n",
    "df_ac = pd.read_csv(attrition,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = selected_columns[:] # Do not select Attrition as a feature node\n",
    "edges = [(\"K\",\"S1\"),\n",
    "         (\"K\",\"S2\"),\n",
    "         (\"K\",\"S3\"),\n",
    "         (\"K\",\"S4\"),\n",
    "         (\"K\",\"S5\"),\n",
    "         (\"K\",\"S6\")]\n",
    "\n",
    "node_colors = { # freestyle, update any color.\n",
    "    \"K\": \"dodgerblue\", \n",
    "    \"S1\":\"lightgreen\", \n",
    "    \"S2\":\"tan\", \n",
    "    \"S3\":\"salmon\",\n",
    "    \"S4\":\"darkcyan\",\n",
    "    \"S5\":\"lightblue\",\n",
    "    \"S6\":\"yellow\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINISHED in 0.062 seconds.\n",
      "Graph named 'Multilingual Out of Vocabulary Terms relationship' with 502 nodes and 693 edges\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1737455/2062787566.py:71: UserWarning: Matplotlib is currently using agg, which is a non-GUI backend, so cannot show the figure.\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "graph_loader = GraphLoader()\n",
    "demo_graph = graph_loader.build_graph(\n",
    "            dataframe = df_attrition.loc[:, selected_columns[:]],\n",
    "            columns = nodes, \n",
    "            edge_list = edges,\n",
    "            verbose = True,\n",
    "            title = 'Multilingual Out of Vocabulary Terms relationship'\n",
    "        )\n",
    "# graph_loader.draw_graph(demo_graph, node_colors, node = 5, radius = 2) # show only the nodes that are at a distance of 2 edges from the employee 5.\n",
    "graph_loader.draw_graph(demo_graph, node_colors) # Show the whole graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "CWD = Path().cwd()\n",
    "EMBEDDINGS_DIR = CWD / 'embeddings'\n",
    "EMBEDDINGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SEED = 12\n",
    "\n",
    "class Line_VectorizerConfig:\n",
    "    dimensions = 128\n",
    "    walk_length = 30\n",
    "    num_walks = 50\n",
    "    window = 10\n",
    "    min_count = 1\n",
    "    batch_words = 100\n",
    "\n",
    "class Line_NodeEmbedding:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.vectorizer = None\n",
    "        self.model = None\n",
    "        \n",
    "    def generate_random_walks(self, graph: nx.Graph, **params) -> None:\n",
    "        self.vectorizer = LINE(graph, **params)\n",
    "        # return self.vectorizer\n",
    "\n",
    "    def fit(self, **params) -> tf.keras.models:\n",
    "        if self.vectorizer is None:\n",
    "            raise Exception(\"No random walks. Generate Random walks by calling generate_random_walks() method first.\")\n",
    "        self.vectorizer.train(**params)\n",
    "        self.model= self.vectorizer.get_embeddings()\n",
    "        return self.model\n",
    "    def save_model(self, model: gensim.models.Word2Vec, save_to: Path = EMBEDDINGS_DIR, prefix: str = None) -> None:\n",
    "        m = gensim.models.keyedvectors.Word2VecKeyedVectors(vector_size=Line_VectorizerConfig.dimensions)\n",
    "        m.add_vectors(list(self.model.keys()), list(self.model.values())) \n",
    "        d = Line_VectorizerConfig.dimensions\n",
    "        w = Line_VectorizerConfig.walk_length\n",
    "        n = Line_VectorizerConfig.num_walks\n",
    "        embeddings_filename = f\"{prefix}_embeddings_{d}_{w}_{n}_line.txt\"\n",
    "        model_filename = f\"{prefix}_model_{d}_{w}_{n}_line.pkl\"\n",
    "        # Save only the embeddings in a txt file.\n",
    "        m.save_word2vec_format(str(EMBEDDINGS_DIR/embeddings_filename))\n",
    "        # Save the entire model.\n",
    "        m.save(str(EMBEDDINGS_DIR/model_filename))\n",
    "        print(f\"Model and embeddings saved to: {str(EMBEDDINGS_DIR/model_filename)}\")\n",
    "    def load_model(self, model_filename: str = None, load_from: Path = EMBEDDINGS_DIR) -> gensim.models.Word2Vec:\n",
    "        if Path(EMBEDDINGS_DIR / model_filename).exists():\n",
    "            print(\"Loaded Model: \", model_filename)\n",
    "            with Path(EMBEDDINGS_DIR / model_filename).open(mode=\"r+b\") as file:\n",
    "                self.model = pickle.load(file)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"NOT found: {EMBEDDINGS_DIR / model_filename}\")\n",
    "        \n",
    "        return self.model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "embedder = Line_NodeEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Administrator\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1969: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.6927\n",
      "Epoch 2/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.6884\n",
      "Epoch 3/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.6723\n",
      "Epoch 4/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.6162\n",
      "Epoch 5/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.4856\n",
      "Epoch 6/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.3138\n",
      "Epoch 7/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.1845\n",
      "Epoch 8/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.1133\n",
      "Epoch 9/50\n",
      "42/42 [==============================] - 0s 2ms/step - loss: 0.0760\n",
      "Epoch 10/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0545\n",
      "Epoch 11/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0408\n",
      "Epoch 12/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0316\n",
      "Epoch 13/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0251\n",
      "Epoch 14/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0204\n",
      "Epoch 15/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0169\n",
      "Epoch 16/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0142\n",
      "Epoch 17/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0121\n",
      "Epoch 18/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0104\n",
      "Epoch 19/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0091\n",
      "Epoch 20/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0079\n",
      "Epoch 21/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0070\n",
      "Epoch 22/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0062\n",
      "Epoch 23/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0056\n",
      "Epoch 24/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0050\n",
      "Epoch 25/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0045\n",
      "Epoch 26/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0041\n",
      "Epoch 27/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0038\n",
      "Epoch 28/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0034\n",
      "Epoch 29/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0031\n",
      "Epoch 30/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0029\n",
      "Epoch 31/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0027\n",
      "Epoch 32/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0025\n",
      "Epoch 33/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0023\n",
      "Epoch 34/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0021\n",
      "Epoch 35/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0020\n",
      "Epoch 36/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0019\n",
      "Epoch 37/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0017\n",
      "Epoch 38/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0016\n",
      "Epoch 39/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0015\n",
      "Epoch 40/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0014\n",
      "Epoch 41/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0014\n",
      "Epoch 42/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0013\n",
      "Epoch 43/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0012\n",
      "Epoch 44/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0011\n",
      "Epoch 45/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0011\n",
      "Epoch 46/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 0.0010\n",
      "Epoch 47/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 9.7115e-04\n",
      "Epoch 48/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 9.2257e-04\n",
      "Epoch 49/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 8.7714e-04\n",
      "Epoch 50/50\n",
      "42/42 [==============================] - 0s 1ms/step - loss: 8.3417e-04\n"
     ]
    }
   ],
   "source": [
    "embedder.generate_random_walks(\n",
    "    demo_graph,\n",
    "    embedding_size = Line_VectorizerConfig.dimensions,\n",
    "    order='second'\n",
    ")\n",
    "\n",
    "model = embedder.fit(\n",
    "    verbose = Line_VectorizerConfig.min_count,\n",
    "    epochs = Line_VectorizerConfig.num_walks,\n",
    "    batch_size = Line_VectorizerConfig.batch_words\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and embeddings saved to: c:\\Users\\Administrator\\Dropbox\\毕业论文\\实验\\embeddings\\K_model_128_30_50_line.pkl\n"
     ]
    }
   ],
   "source": [
    "embedder.save_model(model, save_to = EMBEDDINGS_DIR, prefix = \"K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Model:  K_model_128_30_50_line.pkl\n"
     ]
    }
   ],
   "source": [
    "model2 = embedder.load_model(r\"K_model_128_30_50_line.pkl\", load_from = EMBEDDINGS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'embeddings\\K_embeddings_128_30_50_line.txt', 'r',encoding='UTF-8') as embeddings_file:\n",
    "    embeddings = embeddings_file.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['502 128\\n']"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is requred since embeddings are out of order with the target. They were not matched correctly.\n",
    "def align_features_and_target(df: pd.DataFrame, embeddings_file: str = None):\n",
    "    vectors = []\n",
    "    with Path(EMBEDDINGS_DIR / embeddings_file).open(mode=\"r\",encoding='UTF-8') as file:\n",
    "        results = file.readlines()\n",
    "        for person in df[\"K\"].values:\n",
    "            for line in results[1:]:\n",
    "                if line.split()[0] == str(person):\n",
    "                    vectors.append(line.split()[1:])\n",
    "    \n",
    "    return np.array(vectors).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings_filename = r\"K_embeddings_128_30_50_line.txt\"\n",
    "feature_vectors = align_features_and_target(df_attrition, embeddings_file = embeddings_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 128)"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116,)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_targets = np.array(list(map(lambda label: 1 if label == \"Yes\" else 0, df_attrition[\"K\"])))\n",
    "# node_targets = df_attrition_encoded['Attrition'].values\n",
    "node_targets.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(*kargs): return kargs\n",
    "def rec(S,model):\n",
    "    print('~'*20)\n",
    "    s=S\n",
    "    a=model.similar_by_word(s)\n",
    "    b=[]\n",
    "    \n",
    "    for i in range(len(a)):\n",
    "        try:\n",
    "            b.append([int(a[i][0]),a[i][1]])\n",
    "        except ValueError:\n",
    "            p=1\n",
    "    c=[]\n",
    "    for d in range(len(b)):\n",
    "        c.append(str(df_ac.loc[df_ac['K']==b[d][0]]['word'].values[0]))\n",
    "    print(str(s)+'.'+df_ac.loc[df_ac['K']==s]['word'].values[0]+str(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~~~~~\n",
      "1.백신패스['ワクチンパスポト', '위드코로나', '健康码', 'ウィズコロナ', '독박육아', '走花路', '刷屏', 'マルチ', '完成接种人员', '報復消費']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "2.위드코로나['ウィズコロナ', '奥密克戎毒株', '完成接种人员', '段階日常回復', '접종완료자', '백신패스', 'ヒンナムノー', '독박육아', 'モッパン', '핑프']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "3.접종완료자['完成接种人员', '위드코로나', '奥密克戎毒株', 'ウィズコロナ', '报复性消费', '静かな退職', '走花路', '독박육아', '재택근무', '백신패스']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "4.ウィズコロナ['위드코로나', '段階日常回復', '奥密克戎毒株', '접종완료자', '完成接种人员', '백신패스', '핑프', 'ヒンナムノー', 'オミクロン', '报复性消费']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "5.段階日常回復['ウィズコロナ', '위드코로나', 'ヒンナムノー', '网课大学', '奥密克戎毒株', '在宅勤務', '静かな退職', '만잘부', '完成接种人员', '吃播']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "6.ワクチンパスポト['백신패스', 'ウィズコロナ', '健康码', '居家', '报复性消费', '당모치', '접종완료자', '荒らし', '위드코로나', '完成接种人员']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "7.内卷['노오력', '관둠', '위드코로나', '人流', '재택근무', '静かな退職', '躺平', '伸手党', '自宅療養', '废柴老公']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "8.노오력['内卷', '재택근무', '위드코로나', '静かな退職', '人流', '恐妻家', '관둠', '伸手党', '냥집사', '在宅勤務']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "9.道路族['自宅療養', '위드코로나', '백신패스', '报复性消费', '자택치료', '접종완료자', 'マルチ', '젠더평등', '完成接种人员', '废柴老公']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "10.無策政府['가학방역', '젠더평등', 'メタバース', '废柴老公', '元宇宙', '메타버스', '自宅療養', '지독한놈', '웃안웃', '위드코로나']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "11.가학방역['無策政府', '玻璃心', '妈虫', 'ビジュアル担当', 'ママ虫', 'メタバース', '포스트코로나', '맘충', '팝콘각', '메타버스']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "12.原生家庭['親ガチャ', '접종완료자', '마마보이', '냥집사', '完成接种人员', '재택근무', '居家办公', '위드코로나', '백신패스', '铲屎官']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "13.親ガチャ['재택근무', '原生家庭', '走花路', '마마보이', 'ウェビナー', '居家办公', '在宅勤務', '荒らし', '静かな退職', '독박육아']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "14.자택치료['居家', '自宅療養', '报复性消费', 'ウィズコロナ', '健康码', '后疫情时代', '居家办公', '핑프', '포스트코로나', '메타버스']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "15.自宅療養['居家', '자택치료', 'ポストコロナ', '报复性消费', '위드코로나', '伸手党', '走花路', 'ママ虫', '后疫情时代', '居家办公']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "16.居家['自宅療養', '자택치료', 'ウィズコロナ', 'ワクチンパスポト', '伸手党', '健康码', '报复性消费', 'ポストコロナ', '走花路', '居家办公']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "17.健康码['백신패스', '报复性消费', '위드코로나', 'ワクチンパスポト', '자택치료', 'ウィズコロナ', '居家', '吃播', '핑프', '報復消費']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "18.数媒土著['Z世代', 'MZ세대', '퐁퐁남', '奥密克戎毒株', '메타버스', '刷屏', '위드코로나', 'ウィズコロナ', '元宇宙', '자택치료']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "19.Z世代['数媒土著', 'MZ세대', '메타버스', '元宇宙', '퐁퐁남', '奥密克戎毒株', '刷屏', 'オミクロン', 'マルチ', '后浪']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "20.MZ세대['数媒土著', 'Z世代', '퐁퐁남', '奥密克戎毒株', '메타버스', 'オミクロン', '元宇宙', '자택치료', 'ウィズコロナ', '위드코로나']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "21.관둠['静かな退職', '躺平', '위드코로나', '废柴老公', 'ウィズコロナ', '먹방', '伸手党', '접종완료자', '재택근무', 'オミクロン']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "22.静かな退職['관둠', '躺平', '접종완료자', '在宅勤務', '愛猫家', '废柴老公', '伸手党', '위드코로나', '재택근무', '完成接种人员']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "23.躺平['静かな退職', '관둠', '伸手党', '废柴老公', '당모치', '위드코로나', '玻璃心', '백신패스', 'ダメダン', '报复性消费']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "24.完成接种人员['접종완료자', '위드코로나', 'ウィズコロナ', '奥密克戎毒株', '居家办公', '网课大学', '백신패스', 'マルチ', '报复性消费', '静かな退職']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "25.미접종자['갓생', '花の道', 'メタバース', '젠더평등', '颜值担当', '自宅療養', '메타버스', '居家', 'ワクチンパスポト', '접종완료자']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "26.オミクロン['奥密克戎毒株', 'ウィズコロナ', '위드코로나', '퐁퐁남', '접종완료자', '完成接种人员', '메타버스', '伸手党', '报复性消费', 'ヒンナムノー']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "27.奥密克戎毒株['オミクロン', '위드코로나', 'ウィズコロナ', '접종완료자', '完成接种人员', '퐁퐁남', '数媒土著', 'ヒンナムノー', '刷屏', '走花路']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "28.走花路['花の道', '꽃길', '报复性消费', '독박육아', '颜值担当', '힌남노', '보복소비', '伸手党', '백신패스', '접종완료자']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "29.花の道['꽃길', '走花路', '비담', '颜值担当', '荒らし', 'ビジュアル担当', '백신패스', '在宅勤務', '재택근무', '键盘侠']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "30.꽃길['花の道', '走花路', '재택근무', '荒らし', 'ママ虫', '报复性消费', '颜值担当', '愛猫家', '在宅勤務', '键盘侠']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "31.轩岚诺['힌남노', 'ヒンナムノー', '报复性消费', '먹방', 'ママ虫', '歩きスマホ', '走花路', '元宇宙', '키보드전사', 'ジェンダー平等']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "32.ヒンナムノー['힌남노', '轩岚诺', '위드코로나', '元宇宙', 'ウィズコロナ', '网课大学', '奥密克戎毒株', '먹방', '隔离经济', '神推し']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "33.힌남노['ヒンナムノー', '轩岚诺', '元宇宙', '走花路', '먹방', '메타버스', '吃播', 'モッパン', '网课大学', 'ママ虫']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "34.捂脸笑['웃안웃', '핑프', '后疫情时代', '神推し', '위드코로나', '포스트코로나', '伸手党', 'ウィズコロナ', '재택근무', 'モッパン']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "35.웃안웃['捂脸笑', '废柴老公', '伸手党', '냥집사', '재택근무', '핑프', '躺平', '神推し', '静かな退職', '報復消費']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "36.吃播['モッパン', '먹방', '報復消費', '妈宝男', '위드코로나', '힌남노', '奥密克戎毒株', '健康码', 'マザコン男', '재택근무']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "37.먹방['モッパン', '吃播', '妻管严', '재택근무', '元宇宙', '恐妻家', '居家办公', '伸手党', '废柴老公', 'マルチ']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "38.モッパン['吃播', '먹방', '報復消費', '위드코로나', '走花路', '재택근무', '元宇宙', '핑프', '伸手党', '报复性消费']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "39.퐁퐁남['恐妻家', '妻管严', '메타버스', 'MZ세대', '奥密克戎毒株', '数媒土著', '위드코로나', 'オミクロン', '元宇宙', '먹방']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "40.妻管严['恐妻家', '퐁퐁남', '먹방', '妈宝男', '힌남노', 'ジェンダー平等', '刷屏', '伸手党', '메타버스', '元宇宙']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "41.恐妻家['妻管严', '퐁퐁남', '먹방', '居家办公', '妈宝男', '人流', '위드코로나', '재택근무', '奥密克戎毒株', '在宅勤務']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "42.マザコン男['마마보이', '吃播', '妈宝男', '재택근무', '키보드전사', '居家办公', '在宅ワーク', '在宅勤務', '먹방', '지독한놈']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "43.妈宝男['마마보이', '走花路', '妻管严', '居家办公', '백신패스', '吃播', '恐妻家', 'マルチ', '報復消費', '报复性消费']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "44.마마보이['妈宝男', 'マザコン男', '재택근무', '居家办公', '親ガチャ', '在宅勤務', '위드코로나', '在宅ワーク', '原生家庭', 'ママ虫']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "45.计划人生['人生設計', '당모치', '먹방', '愛猫家', '静かな退職', '伸手党', '재택근무', '꽃길', '키보드전사', '팝콘각']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "46.갓생['人生設計', '꽃길', '元宇宙', '메타버스', '맘충', 'メタバース', 'ママ虫', '재택근무', '静かな退職', '관둠']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "47.人生設計['计划人生', '갓생', '당모치', '재택근무', '静かな退職', '愛猫家', '지독한놈', '伸手党', '꽃길', '元宇宙']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "48.보복소비['报复性消费', '報復消費', '走花路', '꽃길', '后浪', 'ダメダン', 'ポストコロナ', 'モッパン', '포스트코로나', '접종완료자']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "49.报复性消费['보복소비', '報復消費', '走花路', '접종완료자', 'ママ虫', '꽃길', '자택치료', '健康码', '轩岚诺', '독박육아']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "50.報復消費['报复性消费', '보복소비', '핑프', 'モッパン', '먹방', '走花路', '백신패스', '吃播', '网课大学', '위드코로나']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "51.포스트코로나['后疫情时代', '报复性消费', 'ポストコロナ', '위드코로나', '完成接种人员', '自宅療養', 'ウィズコロナ', '자택치료', '보복소비', '접종완료자']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "52.后疫情时代['ポストコロナ', '포스트코로나', '위드코로나', '핑프', '먹방', '自宅療養', 'ウィズコロナ', '자택치료', '伸手党', '백신패스']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "53.ポストコロナ['后疫情时代', '自宅療養', '포스트코로나', '백신패스', '위드코로나', '報復消費', '보복소비', '居家', '먹방', '报复性消费']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "54.큐코노미['隔离经济', '居家办公', '위드코로나', '刷屏', '报复性消费', '在宅勤務', '접종완료자', '재택근무', '人流', '轩岚诺']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "55.隔离经济['큐코노미', '위드코로나', 'ヒンナムノー', '网课大学', '报复性消费', 'ウィズコロナ', '在宅勤務', '奥密克戎毒株', '静かな退職', '废柴老公']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "56.ジェンダー平等['性别平等', '走花路', '轩岚诺', '刷屏', '먹방', '元宇宙', 'モッパン', '퐁퐁남', '妻管严', '젠더평등']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "57.性别平等['ジェンダー平等', '刷屏', 'マルチ', '荒らし', '퐁퐁남', '奥密克戎毒株', '键盘侠', '元宇宙', '妻管严', '恐妻家']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "58.젠더평등['歩きスマホ', 'モッパン', '위드코로나', '접종완료자', 'ジェンダー平等', '백신패스', '走花路', '먹방', '自宅療養', 'マルチ']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "59.인파['메타버스', '居家办公', '人流', '走花路', '접종완료자', '재택근무', 'インフルエンザ-', '奥密克戎毒株', '完成接种人员', '자택치료']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "60.人流['居家办公', '재택근무', '在宅勤務', '위드코로나', '键盘侠', '在宅ワーク', '恐妻家', 'ウィズコロナ', '인파', '親ガチャ']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "61.메타버스['元宇宙', 'メタバース', '퐁퐁남', '힌남노', '인파', '歩きスマホ', '재택근무', 'Z世代', '伸手党', 'オミクロン']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "62.メタバース['메타버스', '玻璃心', '맘충', '재택근무', '元宇宙', '伸手党', 'ママ虫', '접종완료자', '自宅療養', 'ウィズコロナ']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "63.元宇宙['메타버스', '먹방', '힌남노', 'ヒンナムノー', '教えてチャン', 'モッパン', '轩岚诺', 'メタバース', '妈虫', '퐁퐁남']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "64.インフルエンザ-['网红', '위드코로나', '먹방', '인파', 'ウィズコロナ', '走花路', '荒らし', '歩きスマホ', '废柴老公', '键盘侠']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "65.网红['インフルエンザ-', '走花路', '먹방', '위드코로나', '퐁퐁남', '键盘侠', 'ジェンダー平等', 'モッパン', '荒らし', '키보드전사']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "66.맘충['妈虫', 'ママ虫', 'メタバース', '走花路', '재택근무', '꽃길', '키보드전사', '颜值担当', '荒らし', '报复性消费']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "67.妈虫['맘충', 'ママ虫', '居家办公', '元宇宙', 'ビジュアル担当', '재택근무', '메타버스', '꽃길', '荒らし', '在宅勤務']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "68.ママ虫['妈虫', '맘충', '报复性消费', '꽃길', '轩岚诺', '走花路', '스몸비', '재택근무', '居家办公', '自宅療養']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "69.비담['颜值担当', 'ビジュアル担当', '花の道', '铲屎官', '居家办公', '愛猫家', '재택근무', '走花路', '面食い', '在宅勤務']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "70.颜值担当['ビジュアル担当', '비담', '走花路', '花の道', '재택근무', '꽃길', '歩きスマホ', '居家办公', '在宅勤務', '愛猫家']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "71.ビジュアル担当['颜值担当', '비담', '居家办公', '歩きスマホ', '花の道', '在宅勤務', '走花路', '재택근무', '妈虫', '꽃길']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "72.스몸비['歩きスマホ', 'ママ虫', '低头族', '위드코로나', '맘충', '自宅療養', '妈虫', '轩岚诺', '居家办公', 'メタバース']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "73.歩きスマホ['스몸비', '颜值担当', 'ビジュアル担当', '居家办公', '轩岚诺', '먹방', '젠더평등', '재택근무', '메타버스', '위드코로나']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "74.지독한놈['是个狼人', '재택근무', '당모치', '먹방', '愛猫家', '元宇宙', 'モッパン', '居家办公', '팝콘각', '伸手党']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "75.是个狼人['지독한놈', '재택근무', '伸手党', '在宅勤務', '居家办公', '走花路', '당모치', '먹방', '포스트코로나', '在宅ワーク']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "76.냥집사['铲屎官', '愛猫家', '재택근무', '在宅ワーク', '在宅勤務', '백신패스', '颜值担当', 'ビジュアル担当', '静かな退職', '居家办公']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "77.铲屎官['냥집사', '愛猫家', '비담', '백신패스', '面食い', '走花路', '在宅勤務', '颜值担当', '재택근무', '맘충']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "78.愛猫家['在宅勤務', '재택근무', '냥집사', '居家办公', '铲屎官', '静かな退職', '꽃길', '颜值担当', '在宅ワーク', '走花路']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "79.刷屏['マルチ', '백신패스', '奥密克戎毒株', '键盘侠', '먹방', 'ジェンダー平等', '위드코로나', '居家办公', '走花路', '在宅ワーク']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "80.マルチ['刷屏', '백신패스', '먹방', '完成接种人员', '在宅勤務', '위드코로나', '奥密克戎毒株', '键盘侠', '在宅ワーク', 'モッパン']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "81.荒らし['键盘侠', '키보드전사', '꽃길', '花の道', '走花路', '재택근무', '위드코로나', '親ガチャ', '당모치', '맘충']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "82.键盘侠['荒らし', '키보드전사', '재택근무', '꽃길', '在宅勤務', '走花路', '먹방', '人流', 'マルチ', '위드코로나']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "83.키보드전사['荒らし', '键盘侠', '재택근무', '走花路', '轩岚诺', '꽃길', '먹방', '맘충', '在宅ワーク', '在宅勤務']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "84.废柴老公['ダメダン', '먹방', '静かな退職', '居家办公', '在宅勤務', '관둠', '躺平', '위드코로나', '재택근무', '在宅ワーク']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "85.ダメダン['废柴老公', '报复性消费', '보복소비', '관둠', '静かな退職', '隔离经济', '위드코로나', '自宅療養', '핑프', 'ウィズコロナ']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "86.YYDS['神推し', '報復消費', '奥密克戎毒株', '접종완료자', '먹방', '재택근무', '居家办公', '元宇宙', 'ヒンナムノー', '网课大学']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "87.神推し['YYDS', '재택근무', '먹방', '网课大学', '在宅勤務', '접종완료자', '報復消費', '居家办公', '奥密克戎毒株', '完成接种人员']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "88.低头族['스몸비', '歩きスマホ', '위드코로나', 'ママ虫', '轩岚诺', '居家办公', '走花路', '재택근무', '맘충', '妈虫']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "89.식집사['재택근무', '먹방', '팝콘각', '居家办公', '走花路', '꽃길', '是个狼人', '관둠', '당모치', '在宅勤務']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "90.만잘부['在宅勤務', '段階日常回復', '愛猫家', '吃播', '在宅ワーク', '网课大学', 'ウィズコロナ', 'モッパン', '위드코로나', '花の道']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "91.网课大学['줌유니버시티', '完成接种人员', '在宅勤務', '居家办公', '백신패스', 'ヒンナムノー', '위드코로나', '재택근무', '접종완료자', '報復消費']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "92.줌유니버시티['网课大学', '백신패스', '完成接种人员', '접종완료자', '위드코로나', 'モッパン', 'ウィズコロナ', '報復消費', '神推し', '段階日常回復']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "93.在宅ワーク['在宅勤務', '재택근무', '居家办公', '냥집사', '愛猫家', 'マルチ', '먹방', '마마보이', '키보드전사', '废柴老公']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "94.재택근무['居家办公', '在宅勤務', '在宅ワーク', '꽃길', '愛猫家', '먹방', '颜值担当', '人流', '是个狼人', '親ガチャ']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "95.居家办公['재택근무', '在宅勤務', '在宅ワーク', 'ウェビナー', '人流', '愛猫家', '完成接种人员', 'ビジュアル担当', '먹방', '歩きスマホ']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "96.在宅勤務['在宅ワーク', '재택근무', '居家办公', '愛猫家', '静かな退職', '먹방', '网课大学', '꽃길', '人流', 'ビジュアル担当']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "97.豆腐メンタル['쿠크', '走花路', '玻璃心', '轩岚诺', 'ビジュアル担当', '親ガチャ', '居家办公', '奥密克戎毒株', '人流', 'ウィズコロナ']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "98.玻璃心['メタバース', '豆腐メンタル', '网课大学', '위드코로나', '쿠크', '접종완료자', '自宅療養', '妈虫', '伸手党', '재택근무']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "99.쿠크['豆腐メンタル', '键盘侠', '玻璃心', '荒らし', '親ガチャ', '走花路', '人流', '키보드전사', 'ウィズコロナ', '轩岚诺']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "100.伸手党['教えてチャン', '핑프', '走花路', '먹방', '위드코로나', '报复性消费', '静かな退職', '재택근무', 'モッパン', '躺平']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "101.教えてチャン['伸手党', '핑프', '元宇宙', 'ウィズコロナ', '위드코로나', '먹방', 'モッパン', '메타버스', 'メタバース', '재택근무']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "102.핑프['伸手党', '報復消費', '위드코로나', '教えてチャン', 'ウィズコロナ', '后疫情时代', 'モッパン', '走花路', '报复性消费', '먹방']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "103.팝콘각['吃瓜', '당모치', '元宇宙', '伸手党', '재택근무', '먹방', '荒らし', '꽃길', '居家办公', '吃播']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "104.吃瓜['팝콘각', '伸手党', '奥密克戎毒株', '神推し', '報復消費', '먹방', '접종완료자', 'ジェンダー平等', '居家办公', '핑프']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "105.面食い['走花路', '铲屎官', '꽃길', '花の道', '비담', '맘충', '颜值担当', 'ビジュアル担当', '백신패스', '팝콘각']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "106.颜控['독박육아', '走花路', '먹방', '歩きスマホ', 'ウェビナー', '荒らし', '親ガチャ', '神推し', '報復消費', '키보드전사']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "107.丧偶式育儿['독박육아', '접종완료자', '报复性消费', '親ガチャ', '走花路', '居家办公', '完成接种人员', '위드코로나', '在宅ワーク', '网课大学']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "108.독박육아['走花路', '丧偶式育儿', '위드코로나', '백신패스', '报复性消费', '접종완료자', 'ウンオペ', '재택근무', '颜控', '親ガチャ']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "109.ウンオペ['독박육아', '위드코로나', '퐁퐁남', '走花路', '재택근무', '자택치료', '奥密克戎毒株', '먹방', '后疫情时代', '报复性消费']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "110.老铁['먹방', '관둠', '당모치', 'ヒンナムノー', '静かな退職', '元宇宙', '歩きスマホ', '伸手党', '在宅勤務', '玻璃心']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "111.后浪['보복소비', '走花路', '报复性消费', '奥密克戎毒株', '먹방', 'ジェンダー平等', 'モッパン', 'マルチ', '報復消費', '꽃길']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "112.SBN['モッパン', 'メタバース', '恐妻家', '妻管严', '먹방', '伸手党', '妈宝男', '妈虫', '스몸비', 'ウィズコロナ']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "113.黙トレ['위드코로나', '奥密克戎毒株', '完成接种人员', '轩岚诺', '神推し', '网课大学', '퐁퐁남', '低头族', 'ヒンナムノー', '报复性消费']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "114.ウェビナー['居家办公', '재택근무', '親ガチャ', '독박육아', '힌남노', '在宅勤務', '颜控', '歩きスマホ', '走花路', '在宅ワーク']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "115.당모치['꽃길', '팝콘각', '荒らし', '计划人生', '재택근무', '躺平', '静かな退職', '报复性消费', 'ワクチンパスポト', '花の道']\n",
      "~~~~~~~~~~~~~~~~~~~~\n",
      "116.쪄죽따['키보드전사', '당모치', '走花路', 'ワクチンパスポト', '荒らし', '먹방', '재택근무', '마마보이', '键盘侠', '愛猫家']\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,117):\n",
    "    rec(i,model2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deepwalk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from ge.classify import read_node_label, Classifier\n",
    "from ge import DeepWalk\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "CWD = Path().cwd()\n",
    "EMBEDDINGS_DIR = CWD / 'embeddings'\n",
    "EMBEDDINGS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SEED = 12\n",
    "\n",
    "class dw_VectorizerConfig:\n",
    "    dimensions = 128\n",
    "    walk_length = 100\n",
    "    num_walks = 50\n",
    "    window = 10\n",
    "    min_count = 1\n",
    "    batch_words = 100\n",
    "\n",
    "class dw_NodeEmbedding:\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.vectorizer = None\n",
    "        self.model = None\n",
    "        \n",
    "    def generate_random_walks(self, graph: nx.Graph, **params) -> None:\n",
    "        self.vectorizer = DeepWalk(graph, **params)\n",
    "        # return self.vectorizer\n",
    "\n",
    "    def fit(self, **params) -> tf.keras.models:\n",
    "        if self.vectorizer is None:\n",
    "            raise Exception(\"No random walks. Generate Random walks by calling generate_random_walks() method first.\")\n",
    "        self.vectorizer.train(**params)\n",
    "        self.model= self.vectorizer.get_embeddings()\n",
    "        return self.model\n",
    "    def save_model(self, model: gensim.models.Word2Vec, save_to: Path = EMBEDDINGS_DIR, prefix: str = None) -> None:\n",
    "        m = gensim.models.keyedvectors.Word2VecKeyedVectors(vector_size=dw_VectorizerConfig.dimensions)\n",
    "        m.add_vectors(list(self.model.keys()), list(self.model.values())) \n",
    "        d = dw_VectorizerConfig.dimensions\n",
    "        w = dw_VectorizerConfig.walk_length\n",
    "        n = dw_VectorizerConfig.num_walks\n",
    "        embeddings_filename = f\"{prefix}_embeddings_{d}_{w}_{n}_deepwalk.txt\"\n",
    "        model_filename = f\"{prefix}_model_{d}_{w}_{n}_deepwalk.pkl\"\n",
    "        # Save only the embeddings in a txt file.\n",
    "        m.save_word2vec_format(str(EMBEDDINGS_DIR/embeddings_filename))\n",
    "        # Save the entire model.\n",
    "        m.save(str(EMBEDDINGS_DIR/model_filename))\n",
    "        print(f\"Model and embeddings saved to: {str(EMBEDDINGS_DIR/model_filename)}\")\n",
    "    def load_model(self, model_filename: str = None, load_from: Path = EMBEDDINGS_DIR) -> gensim.models.Word2Vec:\n",
    "        if Path(EMBEDDINGS_DIR / model_filename).exists():\n",
    "            print(\"Loaded Model: \", model_filename)\n",
    "            with Path(EMBEDDINGS_DIR / model_filename).open(mode=\"r+b\") as file:\n",
    "                self.model = pickle.load(file)\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"NOT found: {EMBEDDINGS_DIR / model_filename}\")\n",
    "        \n",
    "        return self.model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "embedder2 = dw_NodeEmbedding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    1.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning embedding vectors...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-9:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "Exception in thread Thread-11:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "Exception in thread Thread-10:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870, in run\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/gujiakai/.local/lib/python3.8/site-packages/gensim/models/word2vec.py\", line 1162, in _worker_loop\n",
      "        self.run()\n",
      "  File \"/usr/lib/python3.8/threading.py\", line 870, in run\n",
      "self._target(*self._args, **self._kwargs)\n",
      "      File \"/home/gujiakai/.local/lib/python3.8/site-packages/gensim/models/word2vec.py\", line 1162, in _worker_loop\n",
      "tally, raw_tally = self._do_train_job(data_iterable, alpha, thread_private_mem)\n",
      "  File \"/home/gujiakai/.local/lib/python3.8/site-packages/gensim/models/word2vec.py\", line 951, in _do_train_job\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/gujiakai/.local/lib/python3.8/site-packages/gensim/models/word2vec.py\", line 1162, in _worker_loop\n",
      "    tally += train_batch_sg(self, sentences, alpha, work, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 552, in gensim.models.word2vec_inner.train_batch_sg\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, alpha, thread_private_mem)\n",
      "  File \"/home/gujiakai/.local/lib/python3.8/site-packages/gensim/models/word2vec.py\", line 951, in _do_train_job\n",
      "    tally, raw_tally = self._do_train_job(data_iterable, alpha, thread_private_mem)\n",
      "TypeError  File \"/home/gujiakai/.local/lib/python3.8/site-packages/gensim/models/word2vec.py\", line 951, in _do_train_job\n",
      ": object of type 'int' has no len()\n",
      "    tally += train_batch_sg(self, sentences, alpha, work, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 552, in gensim.models.word2vec_inner.train_batch_sg\n",
      "    tally += train_batch_sg(self, sentences, alpha, work, self.compute_loss)\n",
      "  File \"gensim/models/word2vec_inner.pyx\", line 552, in gensim.models.word2vec_inner.train_batch_sg\n",
      "TypeError: object of type 'int' has no len()\n",
      "TypeError: object of type 'int' has no len()\n"
     ]
    }
   ],
   "source": [
    "embedder2.generate_random_walks(\n",
    "    demo_graph,\n",
    "    walk_length=dw_VectorizerConfig.walk_length,\n",
    "    num_walks=dw_VectorizerConfig.num_walks,\n",
    ")\n",
    "\n",
    "model3 = embedder2.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.save_model(model, save_to = EMBEDDINGS_DIR, prefix = \"K\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "cn",
    "en"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "cn",
   "useGoogleTranslate": true
  },
  "vscode": {
   "interpreter": {
    "hash": "e0144baad0ecee903f108a3e46e51ceadd7da3fc904cfa79747d813b61464b4e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
